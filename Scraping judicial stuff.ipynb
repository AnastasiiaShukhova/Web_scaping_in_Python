{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import os\n",
    "import re\n",
    "import urllib2\n",
    "import csv\n",
    "import codecs\n",
    "import shutil\n",
    "import time\n",
    "import random \n",
    "import csv\n",
    "#kill firefox windows in terminal \n",
    "# pkill -f firefox\n",
    "\n",
    "#set working directory \n",
    "\n",
    "os.chdir('/Users/nastyashukhova/Dropbox/UniMannheim/Thomas/pythonScraping')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to download html code from the cite, and if smth is wrong the function returns the exact error\n",
    "def download(url, user_agent='wswp', num_retries=2):\n",
    "#    print 'Downloading: ...'\n",
    "    headers = {'User-agent': user_agent} # be identifiable as a human\n",
    "    request = urllib2.Request(url, headers=headers)\n",
    "    try:\n",
    "        html = urllib2.urlopen(request).read()\n",
    "#        print 'Downloaded'\n",
    "    except urllib2.URLError as e:\n",
    "        print 'Download error:', e.reason\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                # retry 5XX HTTP errors, because they are connected to the server problems\n",
    "                return download(url, user_agent, num_retries-1) \n",
    "    return html\n",
    "\n",
    "\n",
    "def makeNiceList(text, decoder = None):\n",
    "    if decoder == None:\n",
    "        counter = 0\n",
    "        try:\n",
    "            for name in text:\n",
    "                counter += 1\n",
    "            # make a new string by attaching names to each other after a comma\n",
    "            # if it's the first name, assign its value to new_name\n",
    "                if counter == 1:\n",
    "                    new_nameText = name + ',' + ' '\n",
    "                # if it's the last name, do not attach a comma at the end\n",
    "                elif counter == len(text):\n",
    "                    new_nameText += name\n",
    "                # else: attach a name and a comma\n",
    "                else:\n",
    "                    new_nameText += name + ',' + ' '\n",
    "        except TypeError:\n",
    "            new_nameText = \"None\"\n",
    "    if decoder != None:\n",
    "        counter = 0\n",
    "        try:\n",
    "            for name in str(text).split(\", \"):\n",
    "                counter += 1\n",
    "            # make a new string by attaching names to each other after a comma\n",
    "            # if it's the first name, assign its value to new_name\n",
    "                if counter == 1:\n",
    "                    new_nameText = name.decode(decoder) + ',' + ' '\n",
    "                # if it's the last name, do not attach a comma at the end\n",
    "                elif counter == len(text):\n",
    "                    new_nameText += name.decode(decoder)\n",
    "                # else: attach a name and a comma\n",
    "                else:\n",
    "                    new_nameText += name.decode(decoder) + ',' + ' '\n",
    "        except TypeError:\n",
    "            new_nameText = \"None\"\n",
    "    return(new_nameText)\n",
    "\n",
    "def getUniqueNames(someList):\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for x in someList:\n",
    "        if x not in seen:\n",
    "            uniq.append(x)\n",
    "            seen.add(x)\n",
    "    return uniq\n",
    "\n",
    "\n",
    "#find number of pages\n",
    "def FindNumberOfPages():\n",
    "    searchResultIndex = driver.find_element_by_id(\"searchResultIndex\").text \n",
    "    num = []\n",
    "    if list(searchResultIndex)[-2] == u'\\n': # if number of pages is a one digit number\n",
    "        for i in list(searchResultIndex)[-1:]:\n",
    "            num.append(int(i)) # get the last digit behore u'\\n'\n",
    "        return int(str(num[0])) \n",
    "    \n",
    "    elif list(searchResultIndex)[-3] == u'\\n': # if number of pages is a two digit number\n",
    "        for i in list(searchResultIndex)[-2:]:\n",
    "            num.append(int(i)) # get the last digits behore u'\\n'\n",
    "        return int(str(num[0]) + str(num[1])) # concatenate the the digits\n",
    "    \n",
    "    elif list(searchResultIndex)[-4] == u'\\n': # if number of pages is a one digit number\n",
    "        for i in list(searchResultIndex)[-3:]:\n",
    "            num.append(int(i)) # get the last digits behore u'\\n'\n",
    "        return int(str(num[0]) + str(num[1]) + str(num[2])) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### get the date of the decision\n",
    "# functions that corrects scraped output for the date. \n",
    "# Changes u'dd. Month. yyyy' to 'dd.mm.yyyy'\n",
    "def getDateInNumbers(date):\n",
    "    replacedString = None\n",
    "    monthsString = ['Januar','Februar','April','Mai','Juni','Juli','August','September','Oktober','November','Dezember']\n",
    "    monthsNum = [\"01.\", \"02.\", \"04.\", \"05.\", \"06.\", \"07.\", \"08.\", \"09.\", \"10.\", \"11.\", \"12.\"]\n",
    "    if re.findall(r'(M\\\\xe4rz)', str(date)) == ['M\\\\xe4rz']:\n",
    "        replacedString = re.sub(r'(M\\\\xe4rz)', \"03.\", str(date))\n",
    "        return(replacedString)\n",
    "    else:\n",
    "        monthPattern = re.compile(r'(\\S+)')\n",
    "        month = monthPattern.findall(str(date))\n",
    "        for i in range(0,11):\n",
    "            if month[1] in str(monthsString[i]): # if month is in the list of months defined as monthsString\n",
    "                replacedString = re.sub(monthsString[i], monthsNum[i], str(date))# change month to a number\n",
    "                return(replacedString)\n",
    "    if replacedString == None : # if month is NOT in the list of months defined as monthsString\n",
    "        replacedString = month # then leave as it is\n",
    "        return replacedString\n",
    "\n",
    "\n",
    "def getDateofDecision():\n",
    "    citeClass = soupLink.findAll('p',attrs={'class':'cite'})\n",
    "    #get the text \n",
    "    try:\n",
    "        citeText = citeClass[0].text\n",
    "        #find the place of the date\n",
    "        #set the date pattern \n",
    "        datePattern = re.compile('(\\d+\\.\\s\\S+\\s\\d\\d\\d\\d)', re.I)\n",
    "        #find pattern in the downloaded text\n",
    "        dateResult = datePattern.findall(citeText)\n",
    "        # clean data if possible\n",
    "        dateResult = getDateInNumbers(dateResult)\n",
    "        dateResult = re.sub(\"[(u')\\n\\s+\\[\\]]\", \"\", str(dateResult) )\n",
    "        return(dateResult.decode('utf8'))\n",
    "    except IndexError:\n",
    "        dateResult = 'NA'\n",
    "        \n",
    "\n",
    "##### get number of the senate and chamber \n",
    "## Senat\n",
    "def getNumberofSenat():\n",
    "    citeClass = soupLink.findAll('p',attrs={'class':'cite'})\n",
    "    #get the text \n",
    "    try:\n",
    "        citeText = citeClass[0].text\n",
    "        SenatePattern = re.compile('(des\\s[a-zA-Z]+\\sSenats)|(_+[1-5]_+Senat)', re.I) #set pattern like \"des Zweiten Senats\"\n",
    "        SenateResult = SenatePattern.findall(citeText) # find patter\n",
    "        SenateResult = re.sub(\"[u']\", \"\", str(SenateResult) ) # clean the result\n",
    "        try:\n",
    "            SenateResult = re.split(\"\\s\", str(SenateResult))[1] # get the number of the Senat written in letters (zB Zweiten)\n",
    "            if SenateResult=='Zweiten': #get the number of the Senat\n",
    "                SenateNumber = 2\n",
    "            elif SenateResult=='Ersten':\n",
    "                SenateNumber = 1\n",
    "            else:\n",
    "                try:\n",
    "                    SenateNumber = re.search('[1-5]', str(SenateResult)).group()\n",
    "                except AttributeError:\n",
    "                    SenateNumber = 'NA'\n",
    "        except IndexError:\n",
    "            SenateNumber = 'NA'\n",
    "    except IndexError:\n",
    "        SenateNumber = 'NA'\n",
    "    return(SenateNumber)\n",
    "\n",
    "def getNumberofChamberFromText():\n",
    "    citeClass = soupLink.findAll('p',attrs={'class':'rr1'})\n",
    "    #get the text \n",
    "    try:\n",
    "        citeText = citeClass[0].text\n",
    "        chamberPattern = re.compile(r'([1-5].\\s+Kammer)|(_+[1-5]_+Kammer)', re.I) #set pattern like \"der num. Kammer\"\n",
    "        chamberResult = chamberPattern.findall(citeText) #find pattern in the text \n",
    "        try: \n",
    "            chamberNumber = re.search('[1-5]', str(chamberResult)).group() #get only the number of the chamber\n",
    "        except AttributeError:\n",
    "            chamberNumber = 'NA'\n",
    "    except IndexError:\n",
    "        chamberNumber = 'NA'\n",
    "    return(chamberNumber)\n",
    "\n",
    "def getNumberofChamberFromCitation():\n",
    "    citeClass = soupLink.findAll('p',attrs={'class':'cite'})\n",
    "    #get the text \n",
    "    try:\n",
    "        citeText = citeClass[0].text\n",
    "        chamberPattern = re.compile(r'([1-5].\\s+Kammer)|(_+[1-5]_+Kammer)', re.I) #set pattern like \"der num. Kammer\"\n",
    "        chamberResult = chamberPattern.findall(citeText) #find pattern in the text \n",
    "        try: \n",
    "            chamberNumber = re.search('[1-5]', str(chamberResult)).group() #get only the number of the chamber\n",
    "            chamberNumber = re.sub(r'\\s+', '',str(chamberNumber) )\n",
    "        except AttributeError:\n",
    "            chamberNumber = 'NA'\n",
    "    except IndexError:\n",
    "        chamberNumber = 'NA'\n",
    "\n",
    "    return(chamberNumber)\n",
    "\n",
    "\n",
    "# get the names of the judges \n",
    "def getJudgesFromText_raw():\n",
    "    surnamesOfJudges = []\n",
    "    surnamesClass = soupLink.findAll('p',attrs={'class':'rr2'})\n",
    "    for i in xrange(len(surnamesClass)):\n",
    "        surname = surnamesClass[i].text.encode('utf8')\n",
    "        surname = re.sub(r'[(\\\\\\xc2)&(\\\\\\xa0)&(\\n+)&(\\s+)]', \"_\", str(surname))\n",
    "        surname = re.sub(r'_+', \" \", str(surname))\n",
    "        surname = re.sub(r\"\\[|\\]|'\", \"\", str(surname))\n",
    "        surname = re.sub(r\"\\sund\", \",\", str(surname))\n",
    "        surnamesOfJudges.append(surname)\n",
    "    surnamesOfJudges = re.sub(r\"\\[|\\]|'|(\\\\xa7)\", \"\", str(surnamesOfJudges))\n",
    "    surnamesOfJudges = makeNiceList(surnamesOfJudges, decoder = 'string_escape')\n",
    "    return(surnamesOfJudges)\n",
    "\n",
    "def getJudgesFromText():\n",
    "    surnamesOfJudges = []\n",
    "    surnamesClass = soupLink.findAll('p',attrs={'class':'rr2'})\n",
    "    try:\n",
    "        if len(surnamesClass) > 1:\n",
    "            for i in xrange(len(surnamesClass)):\n",
    "                surname = surnamesClass[i].text.encode('utf8')\n",
    "                surname = re.sub(r'[(\\\\\\xc2)&(\\\\\\xa0)&(\\n+)&(\\s+)]', \"_\", str(surname))\n",
    "                surname = re.sub(r'_+', \" \", str(surname))\n",
    "                surname = re.sub(r\"'|\\[|\\]\", \"\", surname)\n",
    "                surname = re.sub(r'(\\sund)|(\\snd\\s)', ',', str(surname))\n",
    "                surnamesOfJudges.append(surname)\n",
    "            #extract names from the list\n",
    "            extracted_names = []\n",
    "            for part in surnamesOfJudges:\n",
    "                if re.search(r'\\w+', str(part)) == None:\n",
    "                    next\n",
    "                ob = process.extract(part.decode('string_escape'), namesList, limit = 4)\n",
    "                for i in ob:\n",
    "                    if i[1]>75:\n",
    "                        extracted_names.append(i[0])\n",
    "            # clean it \n",
    "            #extracted_names = re.sub(r\"'\", \"\",str(extracted_names))\n",
    "        #return cleaned names \n",
    "        elif len(surnamesClass) == 1:\n",
    "            for i in xrange(len(surnamesClass)):\n",
    "                surname = surnamesClass[i].text.encode('utf8')\n",
    "                surname = re.sub(r'[(\\\\\\xc2)&(\\\\\\xa0)&(\\n+)&(\\s+)]', \"_\", str(surname))\n",
    "                surname = re.sub(r'_+', \" \", str(surname))\n",
    "                surname = re.sub(r\"'|\\[|\\]\", \"\", surname)\n",
    "                surname = re.sub(r'(\\sund)|(\\snd\\s)', ',', str(surname))\n",
    "                surnamesOfJudges.append(surname)\n",
    "            #separate persons from each other\n",
    "            names_separated = re.split(',', str(surnamesOfJudges))\n",
    "            #extract names from the list\n",
    "            extracted_names = []\n",
    "            for part in names_separated:\n",
    "                if re.search(r'\\w+', str(part)) == None:\n",
    "                    next\n",
    "                if re.search(r'(\\\\xa7)', str(part)) != None:\n",
    "                    end = re.search(r'(gem\\\\xc3\\\\xa4\\\\xc3\\\\x9f)', str(part)).start()\n",
    "                    part = part[:end]\n",
    "                ob = process.extract(part.decode('string_escape'), namesList, limit = 4)\n",
    "                for i in ob:\n",
    "                    if i[1]>75:\n",
    "                        extracted_names.append(i[0])\n",
    "            # clean it \n",
    "            #extracted_names = re.sub(r\"'\", \"\",str(extracted_names))\n",
    "        listOfNames = getUniqueNames(extracted_names)\n",
    "        finalListOfNames = makeNiceList(listOfNames)\n",
    "        return(finalListOfNames)\n",
    "    except UnboundLocalError: \n",
    "        return(\"No names in the text\")    \n",
    "    \n",
    "# get the names of the judges \n",
    "def getJudgesFromCitation():\n",
    "    surnamesClass = soupLink.findAll('td',attrs={'class':'st'})\n",
    "    surnamesOfJudges = []\n",
    "    for i in range(len(surnamesClass)):\n",
    "        if re.search(r'ist', str(surnamesClass[i].text.encode('utf8'))) == None: # if the decision was signed normally\n",
    "            surname = surnamesClass[i].text.encode('utf8')\n",
    "            surname = re.sub(r\"(u')\", \"\", str(surname))\n",
    "            surname = re.sub(r'[(\\\\\\xc2)&(\\\\\\xa0)&(\\s+)&(\\n+)]', \"\", str(surname))\n",
    "            surnamesOfJudges.append(surname)\n",
    "# if a judge dropped out, yet was part of decision\n",
    "    if re.search(r'(Der Richter )|(Die Richterin )', str(surnamesClass)): # if instead of the name there is some weird sentence\n",
    "        st = re.search(r'(Der Richter )|(Die Richterin )',  str(surnamesClass)).end() #get the name either after Der Richter or Die Richterin \n",
    "        fin = re.search(r' ',  str(surnamesClass)[st+1:]).end() + st + 1\n",
    "        last_name =  str(surnamesClass)[st:fin]\n",
    "        last_name = re.sub(r\"(u')\", \"\", str(last_name))\n",
    "        last_name = re.sub(r\"[(\\\\\\xc2)&(\\\\\\xa0)&(\\s+)&(\\n+)]\", \"\", str(last_name))\n",
    "        surnamesOfJudges.append(last_name)\n",
    "    if surnamesOfJudges == []:\n",
    "        surnamesOfJudges = None\n",
    "    return(surnamesOfJudges)\n",
    "\n",
    "\n",
    "#get the ID of Beschluss\n",
    "def getID():\n",
    "    idClass = soupLink.findAll('p',attrs={'class':'az2'})\n",
    "    try: \n",
    "        idText = idClass[0].text\n",
    "        idDecision = re.findall(r'\\d+\\s[A-Za-z]+\\s\\d+.\\d+\\s', idText) #set the pattern for the ID\n",
    "        idDecision = re.sub(\"[(u')\\[\\]\\n\\t-]\", \"\", str(idText.encode('utf8'))) # clean it\n",
    "        idDecision = re.sub(r'[(\\\\\\xe2)&(\\\\\\x80)&(\\\\\\x93)|(\\\\\\xc2)&(\\\\\\xa0)\\s+]', \"\", str(idDecision))\n",
    "        return idDecision\n",
    "    except IndexError:\n",
    "        citeClass = soupLink.findAll('p',attrs={'class':'cite'})\n",
    "        try: \n",
    "            idText = citeClass[0].text\n",
    "            idPattern = re.compile('\\d+\\s[A-Za-z]+\\s\\d+.\\d+\\s')#set the pattern for the ID\n",
    "            idDecision = idPattern.findall(idText)\n",
    "            idDecision = re.sub(\"[(u')\\[\\]\\n-]\", \"\", str(idText.encode('utf8'))) # clean it\n",
    "            idDecision = re.sub(r'[(\\\\\\xe2)&(\\\\\\x80)&(\\\\\\x93)|(\\\\\\xc2)&(\\\\\\xa0)\\s+]', \"\", str(idDecision))\n",
    "            return idDecision\n",
    "        except IndexError:\n",
    "            idDecision = 'NA'\n",
    "            return idDecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://www.bundesverfassungsgericht.de/SiteGlobals/Forms/Suche/Entscheidungensuche_Formular.html\"\n",
    "# open the browser\n",
    "driver = webdriver.Firefox()\n",
    "#go the link \n",
    "driver.get(url)\n",
    "yearList = range(1998, 2017)\n",
    "#find the extended search link and click it \n",
    "toExtendSearch = driver.find_element_by_id(\"extendedSearch\")\n",
    "toExtendSearch.find_element_by_link_text(\"Erweiterter Suchbereich\").click()#click to the extended search\n",
    "#set the year span to scrape\n",
    "#from \n",
    "dateAfterClass = driver.find_element_by_class_name(\"toggle\")\n",
    "FindDateAfter = dateAfterClass.find_element_by_name(\"dateAfter\")\n",
    "FindDateAfter.clear() # clear the default value tt.mm.jjj\n",
    "toSetDateAfter = FindDateAfter.send_keys(\"01.01.1998\")\n",
    "\n",
    "# to \n",
    "FindDateBefore = dateAfterClass.find_element_by_name(\"dateBefore\")\n",
    "FindDateBefore.clear() # clear the default value tt.mm.jjj\n",
    "toSetDateBefore = FindDateBefore.send_keys(\"26.04.2016\")\n",
    "\n",
    "#choose the type of the decision \n",
    "select = Select(driver.find_element_by_name('facettedEntscheidungstyp'))\n",
    "selectBeschluss = select.select_by_value(\"Beschluss\")\n",
    "\n",
    "#submit the filled search form \n",
    "driver.find_element_by_class_name(\"submit\").click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we need to click all of the links in the list of the decisions\n",
    "# save all of the links of the decision \n",
    "# get the results of the search \n",
    "\n",
    "# download site for Beautiful Soup\n",
    "soup = download(driver.current_url)\n",
    "soup = BeautifulSoup(soup, 'html.parser')\n",
    "data = soup.findAll('ol',attrs={'id':'searchResult'}) \n",
    "listOfLinks = []\n",
    "\n",
    "#find the links on the first page \n",
    "for div in data:\n",
    "    links = div.findAll(attrs = {'href' : re.compile(r'Entscheidungen')})\n",
    "    for a in links:\n",
    "        listOfLinks.append(\"http://www.bundesverfassungsgericht.de/\" + a['href'])\n",
    "print 'Page 1 out of %i' % ( FindNumberOfPages())\n",
    "        #find the links on the second page to the last one\n",
    "i = []\n",
    "for i in range(1, FindNumberOfPages()-1):\n",
    "    time.sleep(3)\n",
    "    if i != (FindNumberOfPages()):\n",
    "        driver.find_element_by_class_name(\"forward\").click() #click \"forward\" buttom\n",
    "        soup = download(driver.current_url) # get the code of the whole page\n",
    "        soup = BeautifulSoup(soup, 'html.parser') # make soup\n",
    "        data = soup.findAll('ol', attrs={'id':'searchResult'}) #find the result of the search\n",
    "        for div in data: # for child of id = searchResult \n",
    "            links = div.findAll(attrs = {'href' : re.compile(r'Entscheidungen')}) # find all links that concerns the decisions\n",
    "            for a in links:\n",
    "                 listOfLinks.append(\"http://www.bundesverfassungsgericht.de/\" + a['href']) # make them look like normal links\n",
    "        percent = 100*((i+2)/float(FindNumberOfPages()))\n",
    "        print 'Page {0} out of {1}. Downloaded {2} percent'.format(i+1, FindNumberOfPages(), percent )\n",
    "        i+=1\n",
    "    else: #if we are on the last, there is  no need to press forward buttom\n",
    "        # do the same as above\n",
    "        soup = download(driver.current_url) \n",
    "        soup = BeautifulSoup(soup, 'html.parser')\n",
    "        data = soup.findAll('ol', attrs={'id':'searchResult'})\n",
    "        for div in data:\n",
    "            links = div.findAll(attrs = {'href' : re.compile(r'Entscheidungen')})\n",
    "            for a in links:\n",
    "                 listOfLinks.append(\"http://www.bundesverfassungsgericht.de/\" + a['href'])\n",
    "                    \n",
    "links = codecs.open('links.txt', 'w', 'utf-8')\n",
    "links.write('Link')\n",
    "for link in listOfLinks:\n",
    "    links.write(str(link))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"./SavedPages\"\n",
    "listOfPages = os.listdir(path)\n",
    "\n",
    "#for page in listOfPages:\n",
    "#    rename = re.sub(r'[(\\\\\\xe2)&(\\\\\\x80)&(\\\\\\x93)|(\\\\\\xc2)&(\\\\\\xa0)\\s+]' ,'_', str(page))\n",
    "#    os.rename(os.path.join(path, page), os.path.join(path, rename))\n",
    "\n",
    "\n",
    "numbers = re.compile(r'(Page(\\d+)_)')\n",
    "listOfPages = os.listdir(path)\n",
    "\n",
    "errors = []\n",
    "pagesDictionary = {}\n",
    "for page in listOfPages:\n",
    "    try:\n",
    "        pagesDictionary.update({int(numbers.split(page)[2]):page})\n",
    "    except IndexError:\n",
    "        errors.append(page)\n",
    "        next\n",
    "val1 =   pagesDictionary.get(16692)\n",
    "pagesDictionary.update({1669:val1})\n",
    "val2 =   pagesDictionary.get(52621)\n",
    "pagesDictionary.update({5262:val2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = codecs.open('judicialData1.txt', 'w', 'utf-8')\n",
    "output.write('i' + '\\t'+ 'ID' + '\\t'+ 'dateofDecision' + '\\t'+ 'senateNumber' +'\\t'+ 'chamberNumberCitation' +'\\t'+\n",
    "              'chamberNumberText' +'\\t'+'judgesFromText_raw' +'\\t'+'judgesFromText' + '\\t'+'judgesFromCitation' + '\\t'+  \"link\"+ '\\n')    \n",
    "output.close()\n",
    "\n",
    "\n",
    "#create new directory for the saved pages of decisions\n",
    "#os.makedirs('SavedPages')\n",
    "\n",
    "\n",
    "listOfLinks = open('links.txt', 'r')\n",
    "listOfLinks =  listOfLinks.readlines()[1:]\n",
    "\n",
    "#for page in errors:\n",
    "#    os.remove(os.path.join(path, page))\n",
    "#for page in listOfPages:\n",
    "#    if re.search(r\"(_engVersion)\", str(page)) != None:\n",
    "        #os.remove(os.path.join(path, page))\n",
    "    \n",
    "namesList_row = []\n",
    "with open('judges.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        namesList_row.append(row)\n",
    "        #print ', '.join(row)\n",
    "\n",
    "\n",
    "namesList_row = namesList_row[1:]\n",
    "namesList = []\n",
    "for i in namesList_row:\n",
    "    namesList.append(i[1])\n",
    "\n",
    "\n",
    "namesList.append(\"Kessal-Wulf\")\n",
    "namesList.append(\"M\\xc3\\xbcller\")\n",
    "namesList.append(\"Maidowski\")\n",
    "namesList.append(\"K\\xc3\\xb6nig\")\n",
    "#for i in namesList:\n",
    "#    print i \n",
    "print namesList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### WARNING: site blocks after approx. 1000-2000 inquiries.\n",
    "# Need to set time to sleep so that server doesn't blocks you right away or change IP from time to time\n",
    "#go the link \n",
    "listOfLinks = open('links.txt', 'r')\n",
    "listOfLinks =  listOfLinks.readlines()[1:]\n",
    "path = \"./SavedPages\"\n",
    "i  = 1669 \n",
    "Counter = 0\n",
    "for key in sorted(pagesDictionary.keys()):\n",
    "    #Counter += 1\n",
    "    #if Counter > 500:\n",
    "     #   break\n",
    "    page = open(os.path.join(path, pagesDictionary.get(1669)), 'r' )\n",
    "    # make a soup object out of it \n",
    "    # only then run functions I've defined above\n",
    "    soupLink = BeautifulSoup(page, 'html.parser')\n",
    "    dateofDecision = getDateofDecision()\n",
    "    senateNumber = getNumberofSenat()\n",
    "    chamberNumberCitation = getNumberofChamberFromCitation()\n",
    "    chamberNumberText = getNumberofChamberFromText()    \n",
    "    surnamesOfJudgesText_raw = getJudgesFromText_raw()\n",
    "    surnamesOfJudgesText = getJudgesFromText()\n",
    "    surnamesOfJudgesCitation = getJudgesFromCitation()    \n",
    "    ID = getID()\n",
    "    #link = listOfLinks[key]\n",
    "    ### save the data \n",
    "    output = codecs.open('judicialData1.txt', 'a', 'utf-8')\n",
    "    counter = 0\n",
    "    try:\n",
    "        for name in surnamesOfJudgesCitation:\n",
    "            counter += 1\n",
    "        # make a new string by attaching names to each other after a comma\n",
    "        # if it's the first name, assign its value to new_name\n",
    "            if counter == 1:\n",
    "                new_nameCitation = name + ',' + ' '\n",
    "            # if it's the last name, do not attach a comma at the end\n",
    "            elif counter == len(surnamesOfJudgesCitation):\n",
    "                new_nameCitation += name\n",
    "            # else: attach a name and a comma\n",
    "            else:\n",
    "                new_nameCitation += name + ',' + ' '\n",
    "    except TypeError:\n",
    "        new_nameCitation = \"None\"\n",
    "\n",
    "\n",
    "\n",
    "    # Now, write out the new string\n",
    "    output.write(str(i) + '\\t'+ str(ID).decode('utf8') + '\\t'+ str(dateofDecision).decode('utf8') + '\\t'+ \n",
    "                 str(senateNumber) +'\\t'+ str(chamberNumberCitation).decode('utf8') +'\\t'+ str(chamberNumberText)+'\\t'+\n",
    "                 surnamesOfJudgesText_raw.decode('utf8')+'\\t'+ surnamesOfJudgesText.decode('utf8')\n",
    "                 +'\\t' +new_nameCitation.decode('utf8') +'\\t'+ str(listOfLinks[1669]).decode('utf8') )\n",
    "\n",
    "    #output.write(surnamesOfJudgesText.encode('utf-8') +'\\t' +new_nameCitation.decode('utf8') + '\\n' )\n",
    "\n",
    "    output.close()\n",
    "    if i%500 == 0:\n",
    "        percentLink = float(i)/float(len(listOfLinks)) #track the percentage of the downloaded files\n",
    "        print '{0} have already been downloaded'.format(percentLink) \n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # Save pages\n",
    "    try:\n",
    "        pageID = re.search(r'(/\\d+)',  ID).end() #find the last position of ID \n",
    "        pageID =  str(ID)[0:pageID] # get the first ID of the decisions (for the case of multiple ID)\n",
    "        pageID =re.sub(r'[//]', '_', str(pageID)) #change '/' to '_', otherwise python will treat ID as a path \n",
    "    except AttributeError:\n",
    "        pageID = '_engVersion' #get special ID for the translated versions\n",
    "    path = os.path.join('SavedPages', 'Page'+str(link)+str(pageID)+'.txt') # save page with unique name (ID)\n",
    "    with codecs.open(path, \"w+\", 'utf-8') as pageFile:\n",
    "        pageFile.write(soupLink.prettify())\n",
    "    pageFile.close()\n",
    "    percentLink = float(i)/float(len(listOfLinks)) #track the percentage of the downloaded files\n",
    "    print '{0} have already been downloaded'.format(percentLink) \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "with open('judicialData.txt') as old_file:\n",
    "    with open('cleanedJudicialData.txt', 'w') as new_file:\n",
    "        for line in old_file:\n",
    "            try:\n",
    "                if re.search(r'(None)', str(line)).group() != None:\n",
    "                    continue\n",
    "            except AttributeError:\n",
    "                new_file.write( re.sub(r'ÃŸ', 'ss', str(line)))\n",
    "    new_file.close()\n",
    "old_file.close()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"./SavedPages\"\n",
    "listOfPages = os.listdir(path)\n",
    "for page in listOfPages:\n",
    "    rename = re.sub(r'.txt' ,\".html\",str(page))\n",
    "    os.rename(os.path.join(path, page), os.path.join(path, rename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
